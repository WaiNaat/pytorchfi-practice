{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UcQyuCXBv05a"
      },
      "outputs": [],
      "source": [
        "import pytorchfi\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from pytorchfi.core import fault_injection\n",
        "from pytorchfi.neuron_error_models import single_bit_flip_func\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qaDQh5_zwGw0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Q/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 45.2MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "seed = 1234\n",
        "\n",
        "batch_size = 50\n",
        "img_size = 224\n",
        "channels = 3\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "quant_bits = 8\n",
        "layer_type = [torch.nn.Conv2d]\n",
        "\n",
        "imagenet_dataset_dir = 'D:\\dataset'\n",
        "\n",
        "print(use_gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z94o3gBuwK5i"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x233041e9fb0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Original_Model\n",
        "원본 모델에 forward hook을 등록해서 quantization에 필요한 max value를 얻을 수 있도록 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Original_Model(torch.nn.Module):\n",
        "    def __init__(self, model, layer_type, layer_num, *args):\n",
        "        super().__init__(*args)\n",
        "        self.model = model\n",
        "        self.max_value = None\n",
        "        self.layer_type = layer_type\n",
        "        self.layer_num = layer_num\n",
        "        self.fhooks = []\n",
        "\n",
        "        self.layer_cnt = 0\n",
        "        self._register_hook_to_target_layer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.max_value = None\n",
        "        out = self.model(x)\n",
        "        return out, self.max_value.item()\n",
        "    \n",
        "    def forward_hook(self):\n",
        "        def hook(module, input, output):\n",
        "            self.max_value = torch.max(output)\n",
        "        return hook\n",
        "\n",
        "    def _register_hook_to_target_layer(self):\n",
        "        \n",
        "        layer_idx = -1\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if len(list(module.children())) == 0 and type(module) in self.layer_type:\n",
        "                layer_idx += 1\n",
        "\n",
        "                if layer_idx == self.layer_num:\n",
        "                    self.fhooks.append(module.register_forward_hook(self.forward_hook()))\n",
        "                    print(f'Registered forward hook to {module}')\n",
        "\n",
        "        self.layer_cnt = layer_idx + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Fault_Injection_Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Fault_Injection_Experiment:\n",
        "    def __init__(self, model, batch_size, channels, img_size, use_gpu=False, quant_bits=8, seed=1234):\n",
        "        if use_gpu:\n",
        "            model.to(device='cuda')\n",
        "\n",
        "        self.original_model = copy.deepcopy(model)\n",
        "        self.error_model = copy.deepcopy(model)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.channels = channels\n",
        "        self.img_size = img_size\n",
        "        self.use_gpu = use_gpu\n",
        "        self.quant_bits = quant_bits\n",
        "\n",
        "        self.seed = seed\n",
        "\n",
        "    def set_models(self, layer_type, layer_num):\n",
        "        self.layer_type = layer_type\n",
        "        self.layer_num = layer_num\n",
        "        \n",
        "        self.original_model = Original_Model(self.original_model, layer_type, layer_num)\n",
        "        if self.use_gpu:\n",
        "            self.original_model.cuda()\n",
        "        \n",
        "        self.error_model = single_bit_flip_func(\n",
        "            model = self.error_model, \n",
        "            batch_size = self.batch_size, \n",
        "            input_shape = [self.channels, self.img_size, self.img_size], \n",
        "            use_gpu = self.use_gpu,\n",
        "            bits = self.quant_bits,\n",
        "            layer_types = layer_type\n",
        "        )\n",
        "\n",
        "    def run(self, image):\n",
        "        if self.use_gpu:\n",
        "            image = image.to(device='cuda')\n",
        "        \n",
        "        # 원본에 inference 진행\n",
        "        self.original_model.eval()\n",
        "        with torch.no_grad():\n",
        "            original_output, max_value = self.original_model(image)\n",
        "\n",
        "        # injection 위치 지정\n",
        "        random.seed(self.seed)\n",
        "        layer_nums = []\n",
        "        dim1 = []\n",
        "        dim2 = []\n",
        "        dim3 = []\n",
        "\n",
        "        for _ in range(self.batch_size):\n",
        "            layer, C, H, W = pytorchfi.neuron_error_models.random_neuron_location(self.error_model, layer=self.layer_num)\n",
        "            layer_nums.append(layer)\n",
        "            dim1.append(C)\n",
        "            dim2.append(H)\n",
        "            dim3.append(W)\n",
        "\n",
        "        # error model 만들기\n",
        "        self.error_model.set_conv_max([max_value for _ in range(self.error_model.get_total_layers())])\n",
        "\n",
        "        error_model = self.error_model.declare_neuron_fi(\n",
        "            batch = [i for i in range(self.batch_size)],\n",
        "            layer_num = layer_nums,\n",
        "            dim1 = dim1,\n",
        "            dim2 = dim2,\n",
        "            dim3 = dim3,\n",
        "            function = self.error_model.single_bit_flip_signed_across_batch\n",
        "        )        \n",
        "\n",
        "        # error model에 inference 진행\n",
        "        error_model.eval()\n",
        "        with torch.no_grad():\n",
        "            error_output = error_model(image)\n",
        "\n",
        "        return original_output, error_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z7nmuvQjxtOw"
      },
      "outputs": [],
      "source": [
        "# ILSVRC2012 validation set (https://image-net.org/challenges/LSVRC/2012/2012-downloads.php)\n",
        "# normallization info from https://pytorch.org/vision/main/models/generated/torchvision.models.alexnet.html\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "imagenet_dataset = torchvision.datasets.ImageNet(\n",
        "    root = imagenet_dataset_dir,\n",
        "    split = 'val',\n",
        "    transform = preprocess\n",
        ")\n",
        "\n",
        "dataset = torch.utils.data.DataLoader(imagenet_dataset, batch_size = batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NyRjpepFweQy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Program Files\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================ PYTORCHFI INIT SUMMARY ==============================\n",
            "\n",
            "Layer types allowing injections:\n",
            "----------------------------------------------------------------------------------\n",
            "   - Conv2d\n",
            "\n",
            "Model Info:\n",
            "----------------------------------------------------------------------------------\n",
            "   - Shape of input into the model: (3 224 224 )\n",
            "   - Batch Size: 50\n",
            "   - CUDA Enabled: True\n",
            "\n",
            "Layer Info:\n",
            "----------------------------------------------------------------------------------\n",
            "Layer #       Layer type  Dimensions         Weight Shape         Output Shape\n",
            "----------------------------------------------------------------------------------\n",
            "    0           Conv2d           4        [64, 3, 7, 7]    [1, 64, 112, 112]\n",
            "    1           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    2           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    3           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    4           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    5           Conv2d           4      [128, 64, 3, 3]     [1, 128, 28, 28]\n",
            "    6           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "    7           Conv2d           4      [128, 64, 1, 1]     [1, 128, 28, 28]\n",
            "    8           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "    9           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "   10           Conv2d           4     [256, 128, 3, 3]     [1, 256, 14, 14]\n",
            "   11           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   12           Conv2d           4     [256, 128, 1, 1]     [1, 256, 14, 14]\n",
            "   13           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   14           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   15           Conv2d           4     [512, 256, 3, 3]       [1, 512, 7, 7]\n",
            "   16           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "   17           Conv2d           4     [512, 256, 1, 1]       [1, 512, 7, 7]\n",
            "   18           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "   19           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "==================================================================================\n",
            "\n",
            "============================ PYTORCHFI INIT SUMMARY ==============================\n",
            "\n",
            "Layer types allowing injections:\n",
            "----------------------------------------------------------------------------------\n",
            "   - Conv2d\n",
            "\n",
            "Model Info:\n",
            "----------------------------------------------------------------------------------\n",
            "   - Shape of input into the model: (3 224 224 )\n",
            "   - Batch Size: 50\n",
            "   - CUDA Enabled: True\n",
            "\n",
            "Layer Info:\n",
            "----------------------------------------------------------------------------------\n",
            "Layer #       Layer type  Dimensions         Weight Shape         Output Shape\n",
            "----------------------------------------------------------------------------------\n",
            "    0           Conv2d           4        [64, 3, 7, 7]    [1, 64, 112, 112]\n",
            "    1           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    2           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    3           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    4           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    5           Conv2d           4      [128, 64, 3, 3]     [1, 128, 28, 28]\n",
            "    6           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "    7           Conv2d           4      [128, 64, 1, 1]     [1, 128, 28, 28]\n",
            "    8           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "    9           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "   10           Conv2d           4     [256, 128, 3, 3]     [1, 256, 14, 14]\n",
            "   11           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   12           Conv2d           4     [256, 128, 1, 1]     [1, 256, 14, 14]\n",
            "   13           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   14           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   15           Conv2d           4     [512, 256, 3, 3]       [1, 512, 7, 7]\n",
            "   16           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "   17           Conv2d           4     [512, 256, 1, 1]       [1, 512, 7, 7]\n",
            "   18           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "   19           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "==================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# single bit flip을 일으킬 모델 만들기\n",
        "if use_gpu:\n",
        "    model.to(device='cuda')\n",
        "\n",
        "tmp = single_bit_flip_func(\n",
        "    model = copy.deepcopy(model),\n",
        "    batch_size = batch_size, \n",
        "    input_shape = [channels, img_size, img_size], \n",
        "    use_gpu = use_gpu,\n",
        "    bits = quant_bits,\n",
        "    layer_types = layer_type\n",
        ")\n",
        "\n",
        "print(tmp.print_pytorchfi_layer_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered forward hook to Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:10<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #0: 2606 / 34880 = 7.471330275229358%\n",
            "\n",
            "Registered forward hook to Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [07:55<00:00,  2.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #1: 2559 / 34880 = 7.33658256880734%\n",
            "\n",
            "Registered forward hook to Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:04<00:00,  2.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #2: 2524 / 34880 = 7.236238532110091%\n",
            "\n",
            "Registered forward hook to Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:12<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #3: 2533 / 34880 = 7.2620412844036695%\n",
            "\n",
            "Registered forward hook to Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:10<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #4: 2524 / 34880 = 7.236238532110091%\n",
            "\n",
            "Registered forward hook to Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [07:52<00:00,  2.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #5: 2534 / 34880 = 7.264908256880734%\n",
            "\n",
            "Registered forward hook to Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [07:48<00:00,  2.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #6: 2507 / 34880 = 7.187499999999999%\n",
            "\n",
            "Registered forward hook to Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:11<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #7: 2532 / 34880 = 7.259174311926605%\n",
            "\n",
            "Registered forward hook to Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [07:53<00:00,  2.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #8: 2546 / 34880 = 7.299311926605505%\n",
            "\n",
            "Registered forward hook to Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:15<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #9: 2520 / 34880 = 7.224770642201834%\n",
            "\n",
            "Registered forward hook to Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:16<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #10: 2535 / 34880 = 7.267775229357798%\n",
            "\n",
            "Registered forward hook to Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #11: 2501 / 34880 = 7.1702981651376145%\n",
            "\n",
            "Registered forward hook to Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:01<00:00,  1.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #12: 2550 / 34880 = 7.310779816513762%\n",
            "\n",
            "Registered forward hook to Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #13: 2546 / 34880 = 7.299311926605505%\n",
            "\n",
            "Registered forward hook to Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:30<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #14: 2537 / 34880 = 7.2735091743119265%\n",
            "\n",
            "Registered forward hook to Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:59<00:00,  1.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #15: 2532 / 34880 = 7.259174311926605%\n",
            "\n",
            "Registered forward hook to Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:34<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #16: 2565 / 34880 = 7.353784403669724%\n",
            "\n",
            "Registered forward hook to Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [07:45<00:00,  2.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #17: 2511 / 34880 = 7.198967889908257%\n",
            "\n",
            "Registered forward hook to Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [07:22<00:00,  2.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #18: 2542 / 34880 = 7.287844036697249%\n",
            "\n",
            "Registered forward hook to Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [07:23<00:00,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer #19: 2532 / 34880 = 7.259174311926605%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for layer_num in range(tmp.get_total_layers()):\n",
        "\n",
        "    # fi\n",
        "    test = Fault_Injection_Experiment(\n",
        "        model=model, \n",
        "        batch_size=batch_size,\n",
        "        channels=channels, \n",
        "        img_size=img_size, \n",
        "        use_gpu=use_gpu, \n",
        "        quant_bits=quant_bits,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    test.set_models(layer_type=layer_type, layer_num = layer_num)\n",
        "\n",
        "    # run    \n",
        "    orig_correct_cnt = 0\n",
        "    orig_error_diff_cnt = 0\n",
        "\n",
        "    for images, labels in tqdm(dataset):\n",
        "\n",
        "        orig_output, error_output = test.run(images)\n",
        "\n",
        "        orig_output = torch.argmax(orig_output, dim=1).cpu().numpy()\n",
        "        error_output = torch.argmax(error_output, dim=1).cpu().numpy()\n",
        "        labels = labels.numpy()\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            # 원본 모델이 정답을 맞춘 경우\n",
        "            if labels[i] == orig_output[i]:\n",
        "                orig_correct_cnt += 1\n",
        "                # 원본 모델이 정답을 맞췄는데 망가진 모델은 틀린 경우\n",
        "                if orig_output[i] != error_output[i]:\n",
        "                    orig_error_diff_cnt += 1\n",
        "\n",
        "    # save result\n",
        "    result = f'Layer #{layer_num}: {orig_error_diff_cnt} / {orig_correct_cnt} = {orig_error_diff_cnt / orig_correct_cnt * 100}%'\n",
        "    print(result, end='\\n\\n')\n",
        "    results.append(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================ PYTORCHFI INIT SUMMARY ==============================\n",
            "\n",
            "Layer types allowing injections:\n",
            "----------------------------------------------------------------------------------\n",
            "   - Conv2d\n",
            "\n",
            "Model Info:\n",
            "----------------------------------------------------------------------------------\n",
            "   - Shape of input into the model: (3 224 224 )\n",
            "   - Batch Size: 50\n",
            "   - CUDA Enabled: True\n",
            "\n",
            "Layer Info:\n",
            "----------------------------------------------------------------------------------\n",
            "Layer #       Layer type  Dimensions         Weight Shape         Output Shape\n",
            "----------------------------------------------------------------------------------\n",
            "    0           Conv2d           4        [64, 3, 7, 7]    [1, 64, 112, 112]\n",
            "    1           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    2           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    3           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    4           Conv2d           4       [64, 64, 3, 3]      [1, 64, 56, 56]\n",
            "    5           Conv2d           4      [128, 64, 3, 3]     [1, 128, 28, 28]\n",
            "    6           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "    7           Conv2d           4      [128, 64, 1, 1]     [1, 128, 28, 28]\n",
            "    8           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "    9           Conv2d           4     [128, 128, 3, 3]     [1, 128, 28, 28]\n",
            "   10           Conv2d           4     [256, 128, 3, 3]     [1, 256, 14, 14]\n",
            "   11           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   12           Conv2d           4     [256, 128, 1, 1]     [1, 256, 14, 14]\n",
            "   13           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   14           Conv2d           4     [256, 256, 3, 3]     [1, 256, 14, 14]\n",
            "   15           Conv2d           4     [512, 256, 3, 3]       [1, 512, 7, 7]\n",
            "   16           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "   17           Conv2d           4     [512, 256, 1, 1]       [1, 512, 7, 7]\n",
            "   18           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "   19           Conv2d           4     [512, 512, 3, 3]       [1, 512, 7, 7]\n",
            "==================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "save_dir = 'resnet18_conv2d.txt'\n",
        "f = open(save_dir, 'w')\n",
        "\n",
        "f.write(tmp.print_pytorchfi_layer_summary())\n",
        "f.write(f'\\n\\n===== Result =====\\nQuantization bits: {quant_bits}\\n')\n",
        "for result in results:\n",
        "    f.write(result + '\\n')\n",
        "\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
